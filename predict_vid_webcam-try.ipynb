{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e468814",
   "metadata": {},
   "source": [
    "# Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d7f5ca6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T06:41:02.660258Z",
     "start_time": "2022-03-13T06:40:54.611965Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-d2_bozxb because the default path (/home/macot/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import gc\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.get_logger().setLevel('ERROR')\n",
    "# tf.autograph.set_verbosity(2)\n",
    "from tensorflow.keras import models, layers, optimizers, applications\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-perfume",
   "metadata": {},
   "source": [
    "# Use OpenCV to load webcam/video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c61583",
   "metadata": {},
   "source": [
    "## Load the pretrained model\n",
    "\n",
    "The best model was VGG16, and this was used to predict the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "particular-consumer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T06:41:30.966762Z",
     "start_time": "2022-03-13T06:41:19.649803Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 14:41:20.772041: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-13 14:41:27.870813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10419 MB memory:  -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:85:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('savefiles/vgg16_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spectacular-tokyo",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T06:41:41.536511Z",
     "start_time": "2022-03-13T06:41:41.523368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Functional)           (None, None, None, 512)   14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 19)                38931     \n",
      "=================================================================\n",
      "Total params: 14,753,619\n",
      "Trainable params: 14,753,619\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "organizational-acrylic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T06:41:43.075465Z",
     "start_time": "2022-03-13T06:41:43.061675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a',\n",
       " 1: 'ba',\n",
       " 2: 'dara',\n",
       " 3: 'ei',\n",
       " 4: 'ga',\n",
       " 5: 'ha',\n",
       " 6: 'ka',\n",
       " 7: 'kuw',\n",
       " 8: 'la',\n",
       " 9: 'ma',\n",
       " 10: 'na',\n",
       " 11: 'nga',\n",
       " 12: 'ou',\n",
       " 13: 'pa',\n",
       " 14: 'sa',\n",
       " 15: 'ta',\n",
       " 16: 'tul',\n",
       " 17: 'wa',\n",
       " 18: 'ya'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['a', 'ba', 'dara', 'ei', 'ga', 'ha', 'ka', 'kuw', 'la',\n",
    "          'ma', 'na', 'nga', 'ou', 'pa', 'sa', 'ta', 'tul', 'wa', 'ya']\n",
    "label_dict = {i: j for i, j in zip(range(19), sorted(labels))}\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3bbf92",
   "metadata": {},
   "source": [
    "# Predict from Video\n",
    "enter the filepath of the video file in the code `cap = cv2.VideoCapture(filepath)` as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235536a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-13T06:41:53.762Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR:0@60.624] global /io/opencv/modules/videoio/src/cap.cpp (595) open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.5.5) /io/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): demo in function 'icvExtractPattern'\n",
      "\n",
      "\n",
      "2022-03-13 14:41:56.473064: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-03-13 14:42:01.161827: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n",
      "2022-03-13 14:42:07.032874: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-03-13 14:42:07.034457: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-03-13 14:42:07.034560: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas --version\n",
      "2022-03-13 14:42:07.035908: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-03-13 14:42:07.036097: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nga 0.09977549\n"
     ]
    }
   ],
   "source": [
    "## input the filename here\n",
    "cap = cv2.VideoCapture('OpenCV/ako.mp4')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "cap.set(3, width)\n",
    "cap.set(4, height)\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "## THIS WAS ADDED TO TRY TO SAVE THE VIDEO\n",
    "writer = cv2.VideoWriter('demo', \n",
    "                         cv2.VideoWriter_fourcc(*'XVID'),25, (width, height))\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        success, imgOriginal = cap.read()\n",
    "        img = cv2.resize(imgOriginal, (64, 64))\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        \n",
    "        # Write the video\n",
    "        writer.write(imgOriginal) ## THIS WAS ADDED TO SAVE THE VID\n",
    "\n",
    "        #predict\n",
    "        predictions = model.predict(img)\n",
    "        pred_label = label_dict[np.argmax(predictions)]\n",
    "        probVal = np.amax(predictions)\n",
    "        \n",
    "        threshold = 0.80\n",
    "        if probVal > threshold:\n",
    "            cv2.putText(img=imgOriginal, text=pred_label + ': ' + str(probVal),\n",
    "                        org=(100, 100), fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                        fontScale=3, color=(255,255,0), thickness=3)\n",
    "            print(pred_label, probVal)\n",
    "        else:\n",
    "            cv2.putText(img=imgOriginal, text='Unable to predict',\n",
    "                        org=(100, 100), fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                        fontScale=3, color=(255,255,0), thickness=3)\n",
    "            print(pred_label, probVal)\n",
    "\n",
    "        cv2.imshow('BaybAIyin Predictor', imgOriginal)\n",
    "        \n",
    "        time_now = time.time()\n",
    "        num_seconds = time_now - time_start\n",
    "        if num_seconds > 90:\n",
    "            print('Time is up.')\n",
    "            break\n",
    "\n",
    "    except:\n",
    "        print('Error: Closing camera')\n",
    "        success = False\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "    if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "        print('Camera closed by user')\n",
    "        success = False\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "    if not success:\n",
    "        print('Failed to capture image')\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "writer.release() ## THIS WAS ADDED\n",
    "cv2.destroyAllWindows()\n",
    "print('Video End')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb6aea4",
   "metadata": {},
   "source": [
    "# Predict from camera Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95541bf8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-13T06:35:22.435Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Put 0 to acces your webcam\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# if not cap.isOpened():\n",
    "#     print(\"Cannot open camera\")\n",
    "#     exit()\n",
    "\n",
    "# width = 640\n",
    "# height = 480\n",
    "# cap.set(3, width)\n",
    "# cap.set(4, height)\n",
    "\n",
    "# time_start = time.time()\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     try:\n",
    "#         success, imgOriginal = cap.read()\n",
    "#         img = cv2.resize(imgOriginal, (64, 64))\n",
    "#         img = np.expand_dims(img, axis=0)\n",
    "\n",
    "#         #predict\n",
    "#         predictions = model.predict(img)\n",
    "#         pred_label = label_dict[np.argmax(predictions)]\n",
    "#         probVal = np.amax(predictions)\n",
    "        \n",
    "#         threshold = 0.80\n",
    "#         if probVal > threshold:\n",
    "#             cv2.putText(img=imgOriginal, text=pred_label + ': ' + str(probVal),\n",
    "#                         org=(100, 100), fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "#                         fontScale=3, color=(255,255,0), thickness=3)\n",
    "#             print(pred_label, probVal)\n",
    "#         else:\n",
    "#             cv2.putText(img=imgOriginal, text='Unable to predict',\n",
    "#                         org=(100, 100), fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "#                         fontScale=3, color=(255,255,0), thickness=3)\n",
    "#             print(pred_label, probVal)\n",
    "\n",
    "#         cv2.imshow('BaybAIyin Prediction', imgOriginal)\n",
    "        \n",
    "#         time_now = time.time()\n",
    "#         num_seconds = time_now - time_start\n",
    "#         if num_seconds > 90:\n",
    "#             print('Time is up.')\n",
    "#             break\n",
    "\n",
    "#     except:\n",
    "#         print('Error: Closing camera')\n",
    "#         success = False\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "#         break\n",
    "\n",
    "#     if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "#         print('Camera closed by user')\n",
    "#         success = False\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "#         break\n",
    "\n",
    "#     if not success:\n",
    "#         print('Failed to capture image')\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "# print('Camera closed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6c82f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-13T06:35:22.436Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"baybayin2_demo.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee3094c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:.conda-msds2022-ml3]",
   "language": "python",
   "name": "conda-env-.conda-msds2022-ml3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
